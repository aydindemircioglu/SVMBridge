#' Perform an mbo run on a test function and and visualize what happens.
#'
#' Usually used for 1D or 2D examples,
#' useful for figuring out how stuff works and for teaching purposes.
#' Currently only parameter spaces with numerical parameters are supported.
#' For visualization, run \code{autoplot} on the resulting object.
#' What is displayed is documented here: \code{\link{autoplot.MBOExampleRun}}.
#'
#' Please note the following things:
#' - The true objective function (and later everything which is predicted from our surrogate model)
#'   is evaluated on a regular spaced grid. These evaluations are stored in the result object.
#'   You can control the resultion of this grid via \code{points.per.dim}.
#' - In every iteration the fitted, approximating surrogate model is stored in the result object
#'   (via \code{store.model.at} in \code{control}) so we can later visualize it quicky.
#' - To save a few lines of code, you can also pass a test function from package
#'   \code{soobench} to \code{fun}. In this case \code{par.set} and \code{global.opt}
#'   are autogenerated by default and the objective function works even though its
#'   signature does not adhere the \dQuote{parameter must be a list}-specification.
#'
#' @param fun [\code{function}]\cr
#'   Target function. See \code{\link{mbo}} for details. It is also possible to
#'   provide a function from the \code{soobench} package.
#' @template arg_parset
#' @param global.opt [\code{numeric(1)}]\cr
#'   Objective value of global optimum, if known.
#'   \code{NA} means not known, which is the default.
#'   Is used to calculate and display the gap between the optimum
#'   and the current best value in the plot.
#'   Default is \code{NA}.
#' @param learner [\code{\link[mlr]{Learner}}]\cr
#'   See \code{\link{mbo}}.
#'   Default is mlr learner \dQuote{regr.km}, which is kriging from package
#'   DiceKriging, if all parameters are numeric. \code{nugget.estim} is set
#'   to \code{TRUE} depending on whether we have noisy observations or not.
#'   If a least one parameter is discrete the mlr learner \dQuote{regr.randomForest}
#'   from package RandomForest is used as the default.
#' @param control [\code{\link{MBOControl}}]\cr
#'   See \code{\link{mbo}}.
#' @param  points.per.dim [\code{integer}]\cr
#'   Number of (regular spaced) locations at which to
#'   sample the \code{fun} function per dimension.
#'   Default is 50.
#' @param noisy.evals [\code{integer(1)}]\cr
#'   Number of function evaluations per point if \code{fun} is noisy.
#'   Default is 10.
#' @template arg_showinfo
#' @param fun.mean [\code{function}]\cr
#'   True, unnoisy objective fun if \code{fun} is noisy. If not provided some features
#'   will not be displayed (for example the gap between the best point so far and the global optimum).
#' @param ... [any]\cr
#'   Further arguments passed to the learner.
#' @return [\code{MBOExampleRun}]
#' @export
exampleRun = function(fun, par.set, global.opt = NA_real_, learner, control,
  points.per.dim = 50, noisy.evals = 10, show.info = NULL, fun.mean = NULL, ...) {

  assertFunction(fun)
  if (!is.null(fun.mean)) {
    assertFunction(fun.mean)
  }
  assertClass(control, "MBOControl")

  if (missing(par.set) && is_soo_function(fun)) {
    par.set = extractParamSetFromSooFunction(fun)
  }
  assertClass(par.set, "ParamSet")

  if (missing(global.opt) && is_soo_function(fun)) {
    global.opt = global_minimum(fun)$val
  }
  assertNumber(global.opt, na.ok = TRUE)

  par.types = getParamTypes(par.set)

  noisy = control$noisy
  learner = checkLearner(learner, par.set, control, ...)

  points.per.dim = asCount(points.per.dim)
  assertCount(points.per.dim, na.ok = FALSE, positive = TRUE)
  noisy.evals = asCount(noisy.evals)
  assertCount(noisy.evals, na.ok = FALSE, positive = TRUE)
  if (is.null(show.info)) {
    show.info = getOption("mlrMBO.show.info", TRUE)
  }
  assertLogical(show.info, len = 1L, any.missing = FALSE)
  n.params = sum(getParamLengths(par.set))

  if (control$number.of.targets != 1L)
    stopf("exampleRun can only be applied for single objective functions, but you have %i objectives! Use 'exampleRunMultiCrit'!",
      control$number.of.targets)
  if (n.params >= 3L)
    stopf("exampleRun can only be applied for functions with at most 2 dimensions, but you have %iD", n.params)

  control$store.model.at = 0:control$iters
  names.x = getParamIds(par.set, repeated = TRUE, with.nr = TRUE)
  name.y = control$y.name

  if (show.info) {
    messagef("Evaluating true objective function at %s points.",
      if(!noisy) {
        if(n.params == 1) {
          sprintf("%i", points.per.dim)
        } else {
          sprintf("%i x %i", points.per.dim, points.per.dim)
        }
      }
    )
  }

  if (is_soo_function(fun)) {
    fun = makeMBOFunction(fun)
  }

  evals = evaluate(fun, par.set, n.params, par.types, noisy, noisy.evals, points.per.dim, names.x, name.y)
  colnames(evals) = c(names.x, name.y)

  if (is.na(global.opt))
    global.opt.estim = ifelse(control$minimize, min(evals[, name.y]), max(evals[, name.y]))
  else
    global.opt.estim = NA_real_

  #show some info on console
  if (show.info) {
    messagef("Performing MBO on function.")
    messagef("Initial design: %i. Sequential iterations: %i.", control$init.design.points, control$iters)
    messagef("Learner: %s. Settings:\n%s", learner$id, mlr:::getHyperParsString(learner))
  }

  # run optimizer now
  res = mbo(fun, par.set, learner = learner, control = control, show.info = show.info)

  # compute true y-values if deterministic function is known
  y.true = NA
  if (!is.null(fun.mean)) {
    opt.path = as.data.frame(res$opt.path)
    evals.x = opt.path[, names.x, drop = FALSE]
    y.true = apply(evals.x, 1, function(x) fun.mean(as.list(x)))
  }

  makeS3Obj("MBOExampleRun",
    fun.mean = fun.mean,
    par.set = par.set,
    y.true = y.true,
    n.params = n.params,
    par.types = par.types,
    names.x = names.x,
    name.y = name.y,
    points.per.dim = points.per.dim,
    evals = evals,
    global.opt = global.opt,
    global.opt.estim = global.opt.estim,
    learner = learner,
    control = control,
    mbo.res = res
  )
}

#' @export
print.MBOExampleRun = function(x, ...) {
  gap = calculateGap(as.data.frame(x$mbo.res$opt.path), x$global.opt, x$control)
  catf("MBOExampleRun")
  catf("Number of parameters        : %i", x$n.params)
  catf("Parameter names             : %s", collapse(x$names.x))
  catf("Parameter types             : %s", collapse(x$par.types))
  catf("%-28s: %.4e", getGlobalOptString(x), getGlobalOpt(x))
  catf("Gap for best point          : %.4e", gap)
  catf("True points per dim.        : %s", collapse(x$points.per.dim))
  print(x$control)
  catf("Learner                     : %s", x$learner$id)
  catf("Learner settings:\n%s", mlr:::getHyperParsString(x$learner))
  mr = x$mbo.res
  op = mr$opt.path
  catf("Recommended parameters:")
  catf(paramValueToString(op$par.set, mr$x))
  catf("Objective: %s = %.3e\n", op$y.names[1], mr$y)
}

getEvalsForNumeric = function(fun, par.set, noisy, noisy.evals, points.per.dim, names.x) {
  lower = getLower(par.set)
  upper = getUpper(par.set)
  xs = seq(lower, upper, length.out = points.per.dim)
  ys = parallelMap(function(x) {
    if (noisy) {
      mean(replicate(noisy.evals, fun(namedList(names.x, x))))
    } else {
      fun(namedList(names.x, x))
    }
  }, xs, simplify = TRUE)
  evals = data.frame(x = xs, y = ys)
  return(evals)
}

getEvalsFor2dNumeric = function(fun, par.set, noisy, noisy.evals, points.per.dim, names.x, name.y) {
  lower = getLower(par.set)
  upper = getUpper(par.set)
  evals.x = NULL
  eval.x = expand.grid(
    x1 = seq(lower[1], upper[1], length.out = points.per.dim),
    x2 = seq(lower[2], upper[2], length.out = points.per.dim)
  )
  names(eval.x) = names.x
  #print(head(eval.x))
  xs = dfRowsToList(eval.x, par.set)
  ys = parallelMap(function(x) {
    if(noisy) {
      mean(replicate(noisy.evals, fun(x)))
    } else {
      fun(x)
    }
  }, xs, simplify = TRUE)
  evals = cbind(eval.x, y = ys)
  colnames(evals) = c(names.x, name.y)
  return(evals)
}

getEvalsForDiscrete = function(fun, par.set, noisy, noisy.evals, names.x) {
  xs = unlist(par.set$pars[[1]]$values)
  cat(xs, "\n")
  ys = parallelMap(function(x) {
    mean(replicate(noisy.evals, fun(namedList(names.x, x))))
  }, xs, simplify = TRUE)
  evals = data.frame(x = xs, y = ys)
  return(evals)
}

evaluate = function(fun, par.set, n.params, par.types, noisy, noisy.evals, points.per.dim, names.x, name.y) {
  if (n.params == 1L) {
    if (par.types %in% c("numeric", "numericvector")) {
      return(getEvalsForNumeric(fun, par.set, noisy, noisy.evals, points.per.dim, names.x))
    } else if (par.types %in% c("discrete")) {
      if (!noisy) {
        stopf("ExampleRun does not make sense with a single deterministic discrete parameter.")
      }
      return(getEvalsForDiscrete(fun, par.set, noisy, noisy.evals, names.x))
    }
  } else if (n.params == 2L) {
    if (all(par.types %in% c("numeric", "numericvector"))) {
      return(getEvalsFor2dNumeric(fun, par.set, noisy, noisy.evals, points.per.dim, names.x, name.y))
    } else {
      idx.numeric = which(par.types != "discrete")
      lower = getLower(par.set)
      upper = getUpper(par.set)
      x2 = seq(lower, upper, length.out = points.per.dim)

      # get discrete parameter
      idx.discrete = which(par.types == "discrete")
      x1 = unlist(par.set$pars[[idx.discrete]]$values)

      eval.x = expand.grid(x1, x2)

      names(eval.x) = names.x
      xs = dfRowsToList(eval.x, par.set)
      ys = parallelMap(function(x) {
        if(noisy) {
          # do replicates if noisy
          mean(replicate(noisy.evals, fun(x)))
        } else {
          fun(x)
        }
      }, xs, simplify = TRUE)
      evals = cbind(eval.x, y = ys)
      colnames(evals) = c(names.x, name.y)
      return(evals)
    }
  }
}

#' Helper function which returns the (estimated) global optimum.
#'
#' @param run [\code{MBOExampleRun}]\cr
#'   Object of type \code{MBOExampleRun}.
#' @return [\code{numeric(1)}]\cr
#'   (Estimated) global optimum.
#' @export
getGlobalOpt = function(run) {
  ifelse(is.na(run$global.opt), run$global.opt.estim, run$global.opt)
}

getGlobalOptString = function(run) {
  sprintf("Global Opt (%s)",  ifelse(is.na(run$global.opt), "estimated", "known"))
}
