\documentclass[article, shortnames]{jss}

% TODO:
% \proglang{C++}/R -> prolang{.}
% SVMBridge -> pkg{.}


\usepackage{amsmath, amssymb}
\usepackage{mathtools}

\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Order}{\mathcal{O}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% almost as usual
\author{Ayd\i n Demircio\u{g}lu\\Ruhr-Universit\"at Bochum \And 
Hanna Houphouet\\Ruhr-Universit\"at Bochum \And 
        Daniel Horn\\TU Dortmund \AND
        Tobias Glasmachers\\Ruhr-Universit\"at Bochum \And 
        Bernd Bischl\\TU M\"unchen \And 
        Claus Weihs\\TU Dortmund}
\title{\pkg{SVMBridge}: Using SVM Solvers from R}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Ayd\i n Demircio\u{g}lu, Daniel Horn} %% comma-separated
\Plaintitle{SVMBridge: Using SVM Solvers from R} %% without formatting
\Shorttitle{\pkg{SVMBridge}: Using SVM Solvers from R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  Most SVM Solver do not come with a wrapper in  \proglang{R}.
  In case the SVM Solver is written in \proglang{C++}, it can be linked to
  a package via  Rcpp. Although possible, this entails a lot of work to do,
  and is not possible, if the SVM Solver is written in other languages.
  Alternatively, one can call the SVM Solver from within \proglang{R} by a system command.
  SVMBridge eases this calls by providing a framework and ready wrappers
  for several SVM Solvers like LASVM, SVMperf, LLSVM and BVM/CVM.  
}
\Keywords{support vector machines, command line}
\Plainkeywords{support vector machines, command line} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Ayd\i n Demircio\u{g}lu, Hanna Houphouet, Tobias Glasmachers\\
  Institut f\"ur Neuroinformatik\\
  Ruhr-Universit\"at Bochum\\
  44790 Bochum\\
  Germany\\
  E-mail: \email{\{aydin.demircioglu, hanna.houphouet, tobias.glasmachers\}@ini.rub.de}\\
  URL: \url{http://www.ini.rub.de}\\
  \\
  
  Daniel Horn, Claus Weihs\\
  Fakult\"at Statistik\\
  Technische Universit\"at Dortmund\\
  44221 Dortmund\\
  Germany \\ 
  E-mail: \email{\{dhorn, bischl, weihs\}@statistik.tu-dortmund.de} \\
  URL: \url{https://www.statistik.tu-dortmund.de/computationalstats.html}\\
\\

  Bernd Bischl\\
Institut f\"ur Statistik \\
Ludwig-Maximilians-Universit\"at M\"unchen\\
80539 M\"unchen\\
  Germany \\
E-mail: \email{bernd.bischl@stat.uni-muenchen.de}\\
URL: \url{http://www.statistik.lmu.de/~bischl}
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}




%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%% Note: If there is markup in \(sub)section, then it has to be escape as above.

\section{Introduction}



\subsection{Support Vector Machines}

Support Vector Machines (SVM) are linear classifiers
that try to maxime the margin of a binary problem \citep{cortes1995support}.
As SVMs are only linear classifiers, a kernelized version
is used for more complex data. Basically, they work by
solving the  following convex problem:
\begin{equation}
        \min_{w \in \mathcal{H}, b \in \R} \quad \frac{1}{2} ||w||^2 + C \cdot \sum_{i=1}^n \max \Big( 0, 1 - y_i \big( \langle w, \varphi(x_i) \rangle_{\mathcal{H}} + b \big) \Big).
        \label{SVMproblemPrimal}
\end{equation}
Here 
$\mathcal{D}=\{(x_1, y_1), \dots, (x_n, y_n)\} \in (X \times \{ \pm 1\})^n$
is the labeled data,
and  $\varphi : X \to \mathcal{H}$ is a feature map into a reproducing
kernel Hilbert space $\mathcal{H}$, corresponding to a positive definite
(Mercer) kernel function $k : X \times X \to \R$, fulfilling
$\langle \varphi(x), \varphi(x') \rangle_{\mathcal{H}} = k(x, x')$ for
all $x, x' \in X$. Furthermore, $C > 0$ is a regularization parameter.
It controls the complexity of the SVM model.

Often, the problem is not directly solved, but dualized via Lagrangian theory.
Then the problem can be stated as 
\begin{align}
        \max_{\alpha \in \R^n} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j k(x_i, x_j) \label{SVMproblemDual} \\
        \text{s.t.} \quad & \sum_{i=1}^{n} y_i \alpha_i = 0 \text{~~and~~} 0 \leq \alpha_i \leq C \,\,\, \forall \, i \in \{1, \dots, n\}. \notag
        \enspace
\end{align}
The solution then takes the form $w = \sum_{i=1}^n \alpha_i y_i \varphi(x_i)$
and the offset~$b$ can be computed from the Karush-Kuhn-Tucker (KKT)
complementarity conditions.

SVMs classify by using the model given by
$h_{w,b}(x) = \textrm{sign}(\langle w, \phi(x)\rangle_{\mathcal{H}} + b)$

Different kernels can be used, e.g. the polynomial kernel or the 
RBF kernel $k(x,x') = e^{-\gamma || x-x' ||^2}$, where
$\gamma$ and $d$ are parameters of the kernel, that must
be tuned to the given data.

SMO.

On the practical side, LIBSVM can be regarded as the 
reference implementation of the SMO and the 'golden standard'
of SVM solvers \citep{cc01a}.


\subsection[SVM packages in R]{SVM packages in \proglang{R}}

There are only a few SVM libraries readily available from within \proglang{R}.
This includes the well-known packages like \pkg{kernlab} (via \code{ksvm()})~\citep{kernlab}, 
and \pkg{e1071} (via \code{svm()})~\citep{dimitriadou2008misc}
as well as other less known ones, e.g. \pkg{SwarmSVM}~\citep{SwarmSVM} 
and \pkg{lasvmR}~\citep{lasvmR}.
All of these link existing \proglang{C++} sources directly from within \proglang{R} and need
more or less extensive wrappers to allow the user direct access to
the options of the underlying SVM solver.

Other SVM solvers, like the BudgetedSVM package~\citep{djuric2014budgetedsvm}, 
which contains BSGD and LLSVM, or SVMperf~\citep{joachims2009sparse}, cannot be called directly from  \proglang{R}.
Instead, a system command has to be issued.
To ease this calls, the \pkg{SVMBridge} package has been developed.



\section{The SVMBridge Package}

The \pkg{SVMBridge} provides a framework to incorporate 
SVM solvers by means of direct calls (via \code{system2()} commands).
This is different to \pkg{e1071} and \pkg{kernlab}, which link (and
thus copy) existing \proglang{C++} code directly inside the package.
\pkg{SVMBridge} works thus differently: Each SVM solver must provide
its own socalled {\it Wrapper}. This is a piece of R code that contains
code e.g. to allow the proper calling of the system binary of a SVM solver
as well as handling the corresponding models. Such wrapper can be
registered in the SVMBridge and henceforth can be used opaquely.



\subsection{Wrapper}

A wrapper is a S3 object: It must provide routines
for assembling the command line call for training as well as testing,
reading and writing the model and a few general routines like searching
the binaries and printing.

Note that the search for binaries might have a huge toll on HPC clusters,
so it is adviced to specify the paths directly instead of relying on the 
automatism.

We provided default wrappers for LIBSVM, LASVM, CVM/BVM,
SVMperf, BSGD and LLSVM. 



\section{Using a Wrapper}

To use the SVMBridge, one usually perform the following steps:
Add the external wrapper to the SVMBridge, load the data, train a model, use the
model for prediction.

Notice that the data must be written by the SVMBridge before calling 
the corresponding SVM Solver, as there is no way to pass data via memory
to command line tools. Therefore 


\subsection{Adding the Wrapper to the SVMBridge}

The first step is to make the SVMBridge aware of a new wrapper.
This is simply done by calling \code{addSVMPackage()}.
There are several options.
The SVMBridge comes with an easy mechanism to search 
the corresponding binaries (specified in the wrapper) to ease
the usage. To use this, one can instead call \code{findSVMSoftware}.



\subsection{Reading Sparse Data}

Most SVM solver work with the sparse data format.
This format consists of:
Each line (delimited by a CR/LF) is given by a label
and the non-zero components of the data point,
e.g. to encode the vector six dimensional vector 
 $ ( 0 , 0 , 0 , 0.2 , 0 , 1.4 ) $
belonging to class $3$, the sparse 
data format would contain the line {3 4:0.2 6:1.4}.

Although reading these files into \proglang{R} is possible via either
the \pkg{kernlab} or the \pkg{e1071} routines, both only provide \proglang{R} solutions,
and therefore suffer from suboptimal performance.
The SVMBridge package provides a simple sparse data format 
reading and writing, which are implemented in \proglang{C++} and
therefore are nearly two orders of magnitudes faster than
the corresponding e1071 routines.
Notice, that currently only dense matrices are supported,
TODO: work around this with the Sparse Matrix Package.



\subsection{Training a model}

Multiclass is supported, where possible.
Note that several SVM packages only support binary problems.
In these cases, training a one-vs-all machine is rather easy, if the
data is loaded into \proglang{R} first. 


Reading a model is possible via the \code{readModelFromFile ()} function.
This will try to detect the format of the file by calling the 
\code{isModelFile} routine of each known wrapper.
Unluckily, several SVM solver use similar model formats, so that
a direct detection is not possible, e.g. CVM/BVM follow the LIBSVM
format, but add a comment (with '\#')about the running time to the
bottom of the model file. Apart from this change, there is no difference.
From a practical viewpoint we extended the LIBSVM model reader
to cope with this extra line, so that there is no need for an extra CVM/BVM reader.
This means that reading a CVM model will make the SVMBridge to detect a LIBSVM
model (if the LIBSVM wrapper is loaded).
In these cases, if multiple models claims ownership, a "default" model can be provided,
which will take precedence over other models. Without a default,
the model will be random.

\begin{CodeChunk}
\begin{CodeInput}
R> model = trainSVM (method = "LIBSVM", cost = 1.0, gamma = 2.0,
			trainDataFile = "./data.sparse",
			epsilon = 0.042,
			modelFile = modelFile,
			useBias = TRUE,
			verbose = verbose
		)
\end{CodeInput}
\begin{CodeOutput}
...
\end{CodeOutput}
\end{CodeChunk}

The training data can be passed on via a variable in memory or by specifying the path 
of the data set file. 
Note that there are two sets of variables: Those who belong to the SVMBridge,
like the trainDataFile variable or verbose flag, and those that are passed further
to the underlying SVM wrapper.
Not all options that are provided by the underlying SVM solver
are supported by the wrappers. As our focus lies on binary classification, because of time constraints 
we opted  to support only these options, e.g. we dropped the one-class SVDD in LIBSVM.
In case there is need for other options, it is 
easy to enhance the wrappers.
There is furthermore the subsampling option, which can be used for larger data sets.


\subsection{Predicions}

Predicting from a trained model is rather easy, as there are usually not many options.
As with training, the predict routine will accept a model either in memory or from a file.
The same is true for the data to predict.  Again notice that data and models in memory 
must be written to disk prior to calling the prediction binary.

\begin{CodeChunk}
\begin{CodeInput}
R> predictions = testSVM (model, data = '')
R> head(predictions)
\end{CodeInput}
\begin{CodeOutput}
...
\end{CodeOutput}
\end{CodeChunk}


\subsection{Optimization Values}

At times, some of the training values are of interest,
e.g. when comparing different solvers, the primal value
of the SVM problem as well as the dual value might  be of interest.
These can be computed via the \code{optimizationValues()} routine.

Specifically, this routine compute the following values:
\code{weight} is given by $\frac{1}{2} w^T w$, with
$w = \sum_i \alpha_i s_i$. Furthermore, we have
that $C \sum_i max (0, 1 - ..) $, and 
\code{primal} is then given by $primal = w + CHinge$.
\code{dual} on the other hand is simply
$w - \sum_i |\alpha_i|$. In general it must hold
that $primal \geq dual$. 
WHAT IF NOT? what does it mean?


\subsection{Helper  Routines}

A few routines have been placed into the package that were helpful
with testing. Namely, there are several generators for synthetical data sets,
collected from different publications. 
WHAT ELSE? can routines be used outside? detectTilde e.g.?


\subsection{Platform Considerations}

Although the SVMBridge was meant to be cross-platform, this goal
is hard to achieve. From a users perspective, it can be used on all three
major platforms (Linux, MacOS, Windows). Additional tests should
be started to make sure the package works as intended.


\subsection{Performance Considerations}

We sum up the points to keep in mind when performance is of high priority:
Do not use the automatism to find the binaries, specify the paths by hand.
Do not load the data into memory, specify the path of the data when training instead.
Do not re-read the model into memory, let it on disk.



\section{Creating a Wrapper}

Adding your own SVM solver to the SVMBridge boils down to
writing a S3 class with several routines. In the inst folder you will find
a template that you can fill with your own code. Here we will go
through the details of adding the software package ...

The flow is as follows: The SVMBridge will call 
createTrainingArguments method of any wrapper. The wrapper
will return a string that contains all parameters that need to be
passed to the binary of the underlying SVM solver. Note that
this includes the model, prediction and training files.



\subsection{Training Call}

A simple S3 method has to be created that needs to be called
\code{createTrainingArguments.LIBSVM}. Apart from the very
first argument (taking the object itself), all other paramters
can be chosen freely. After training, from the output several
training information will be extracted by the 
\code{extractTrainingInfo()} function. 


\subsection{Testing Call}

Similar to the training call, \code{createTestArguments()} needs to
writen. Again, it should accept three variables called testDataFile and
modelFile as well as predictionFile. There is also a
\code{extractTestInfo()} that will extract the important informations
from the output of the test binary.



\subsection{Handling Models}

As only LIBSVM models are being used, one can readily use the 
inbuild functions of the SVMBridge. If the solver works with its
own model, one needs to rewrite the readModel method of the wrapper.
This will only obtain the modelFile and will return the model
in the model format (S3 Object with XY, describe it somewhere else)
SVMBridge needs also a writeModel functions, as models passed in
memory needs to be written to disk. .
Finally it is necessary to access the predictions itself. For this one
needs to write the \code{readPredictions()} function. For LIBSVM this
will read the prediction files that is being written by LIBSVM to disk.


\subsection{Other functions}

Every wrapper should also be able to find itself in a given directory tree.
For this the S3 method \code{findSoftware()} must be provided.

At last, for querying reasons, there is also a print routine
so that wrapper can print out any further information about itself.
To see the currently loaded wrapper, one can call 
\code{overviewAllSVMSolver()}.


\section{Conclusion}

We provided a simple framework, \pkg{SVMBridge},  to attach many SVM solvers to the \proglang{R} landscape.
\pkg{SVMBridge} does this by an easily extensible wrappers that manage the exchange of data and models
via command line.
By providing such a wrapper, linking any SVM solver to \proglang{R} becomes much easier.



\section*{Acknowledgments}

We acknowledge support by the Mercator  \proglang{R}.search Center  \proglang{R}.hr,
under grant Pr-2013-0015 \textit{Support-Vektor-Maschinen f{\"u}r extrem gro{\ss}e Datenmengen} and
partial support by the German  \proglang{R}.search Foundation (DFG) within the Collaborative  \proglang{R}.search Centers SFB 823
\textit{Statistical modelling of nonlinear dynamic processes}, Project C2.


\bibliography{collected}

\end{document}
